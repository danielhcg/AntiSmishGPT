{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMJpLjzIdZuEBZTaXGmM9bY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielhcg/AntiSmishGPT/blob/main/llama_smishTank_Testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQW3lM4jkpNP",
        "outputId": "61f225dd-3023-4bfb-ffa8-0bfd36dd6400"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llamaapi\n",
            "  Downloading llamaapi-0.1.36-py3-none-any.whl (4.0 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.5 in /usr/local/lib/python3.10/dist-packages (from llamaapi) (3.9.5)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from llamaapi) (1.6.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.27.1 in /usr/local/lib/python3.10/dist-packages (from llamaapi) (2.31.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.5->llamaapi) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.5->llamaapi) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.5->llamaapi) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.5->llamaapi) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.5->llamaapi) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.5->llamaapi) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.27.1->llamaapi) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.27.1->llamaapi) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.27.1->llamaapi) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.27.1->llamaapi) (2024.2.2)\n",
            "Installing collected packages: llamaapi\n",
            "Successfully installed llamaapi-0.1.36\n"
          ]
        }
      ],
      "source": [
        "pip install llamaapi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tqdm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MvQyoGqo7Kl",
        "outputId": "01fd1ae0-0f99-40b9-aa7b-b58f930dc5d3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from llamaapi import LlamaAPI\n",
        "from google.colab import userdata\n",
        "from tqdm import tqdm\n",
        "import re  # Ensure 're' module is imported\n",
        "import time\n",
        "\n",
        "\n",
        "# Initialize the SDK\n",
        "llama = LlamaAPI(userdata.get('LLAMA-API-KEY'))\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Pjli20Sxkyd0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_api(message):\n",
        "    response = llama.run({\"messages\": [{\"role\": \"user\", \"content\": message}]}).json()\n",
        "    content = response['choices'][0]['message']['content']\n",
        "    print(f\"API response content: {content}\")\n",
        "\n",
        "# Test message\n",
        "message = \"Costco: Daniel, the code 42003 printed on your receipt from 10 came in 2nd in our Airpods draw: f2gpy.info/RzNKEws Zve\"\n",
        "test_message = f\"Is this message real or fake. Just say real, fake, or undetermined: {message}\"\n",
        "test_api(test_message)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jFgfSLXp3y6",
        "outputId": "b478b749-8206-4ad3-9190-e223bcf9dbba"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API response content: Fake\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def check_sms_legitimacy(message):\n",
        "    request_message = f\"Is this message real or fake. Just say real, fake, or undetermined: {message}\"\n",
        "    response = llama.run({\"messages\": [{\"role\": \"user\", \"content\": request_message}]}).json()\n",
        "    model = response.get('model', 'Unknown model')\n",
        "    content = response['choices'][0]['message']['content']\n",
        "    return content, model\n",
        "\n",
        "# Read messages from an Excel file\n",
        "input_file = 'sms_messages.xlsx'\n",
        "df = pd.read_excel(input_file, header=None)\n",
        "df.columns = ['message']\n",
        "\n",
        "# Process each message and get legitimacy determination\n",
        "determinations = []\n",
        "models = []\n",
        "for i, message in enumerate(df['message'], start=1):\n",
        "    determination, model = check_sms_legitimacy(message)\n",
        "    determinations.append(determination)\n",
        "    models.append(model)\n",
        "    print(f\"Message {i}: {determination} (Model: {model})\")\n",
        "\n",
        "# Add the determinations to the DataFrame\n",
        "df['determination'] = determinations\n",
        "df['model'] = models\n",
        "\n",
        "# Save the results to a new Excel file\n",
        "output_file = 'sms_legitimacy_results.xlsx'\n",
        "df.to_excel(output_file, index=False)\n",
        "\n",
        "print(\"Legitimacy check completed. Results saved to:\", output_file)"
      ],
      "metadata": {
        "id": "xpj6JK04eUxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this function keeps retryin until a determination is found\n",
        "def check_sms_legitimacy(message):\n",
        "    request_message = f\"Is this message real or fake. Just say real, fake, or undetermined: {message}\"\n",
        "    while True:\n",
        "        try:\n",
        "            response = llama.run({\"messages\": [{\"role\": \"user\", \"content\": request_message}]})\n",
        "\n",
        "            try:\n",
        "                response_json = response.json()\n",
        "                model = response_json.get('model', 'Unknown model')\n",
        "                content = response_json['choices'][0]['message']['content']\n",
        "                return content, model\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(\"Error decoding JSON:\", e)\n",
        "                print(\"Raw response text:\", response.text)\n",
        "                time.sleep(1)  # Wait for a second before retrying\n",
        "        except Exception as e:\n",
        "            print(\"Error during API request:\", e)\n",
        "            time.sleep(1)  # Wait for a second before retrying\n",
        "\n",
        "# Read messages from an Excel file\n",
        "input_file = 'sms_messages.xlsx'\n",
        "df = pd.read_excel(input_file, header=None)\n",
        "df.columns = ['message']\n",
        "\n",
        "# Process each message and get legitimacy determination\n",
        "determinations = []\n",
        "models = []\n",
        "for i, message in enumerate(df['message'], start=1):\n",
        "    determination, model = check_sms_legitimacy(message)\n",
        "    determinations.append(determination)\n",
        "    models.append(model)\n",
        "    print(f\"Message {i}: {determination} (Model: {model})\")\n",
        "\n",
        "# Add the determinations to the DataFrame\n",
        "df['determination'] = determinations\n",
        "df['model'] = models\n",
        "\n",
        "# Save the results to a new Excel file\n",
        "output_file = 'sms_legitimacy_results.xlsx'\n",
        "df.to_excel(output_file, index=False)\n",
        "\n",
        "print(\"Legitimacy check completed. Results saved to:\", output_file)"
      ],
      "metadata": {
        "id": "ERBeYW-gnSk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_sms_legitimacy(message, model):\n",
        "    request_message = f\"Is this message real or fake. Just say real, fake, or undetermined: {message}\"\n",
        "    while True:\n",
        "        try:\n",
        "            response = llama.run({\n",
        "                \"model\": model,\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": request_message}]\n",
        "            })\n",
        "\n",
        "            try:\n",
        "                response_json = response.json()\n",
        "                used_model = response_json.get('model', 'Unknown model')\n",
        "                content = response_json['choices'][0]['message']['content']\n",
        "                return content, used_model\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(\"Error decoding JSON:\", e)\n",
        "                print(\"Raw response text:\", response.text)\n",
        "                time.sleep(1)  # Wait for a second before retrying\n",
        "        except Exception as e:\n",
        "            print(\"Error during API request:\", e)\n",
        "            time.sleep(1)  # Wait for a second before retrying\n",
        "\n",
        "# Specify the model you want to use\n",
        "model_to_use = \"llama3-70b\"  # Replace with your desired model\n",
        "\n",
        "# Read messages from an Excel file\n",
        "input_file = 'sms_messages.xlsx'\n",
        "df = pd.read_excel(input_file, header=None)\n",
        "df.columns = ['message']\n",
        "\n",
        "# Process each message and get legitimacy determination\n",
        "determinations = []\n",
        "models = []\n",
        "for i, message in enumerate(df['message'], start=1):\n",
        "    determination, model = check_sms_legitimacy(message, model_to_use)\n",
        "    determinations.append(determination)\n",
        "    models.append(model)\n",
        "    print(f\"Message {i}: {determination} (Model: {model})\")\n",
        "\n",
        "# Add the determinations to the DataFrame\n",
        "df['determination'] = determinations\n",
        "df['model'] = models\n",
        "\n",
        "# Save the results to a new Excel file\n",
        "output_file = 'sms_legitimacy_results.xlsx'\n",
        "df.to_excel(output_file, index=False)\n",
        "\n",
        "print(\"Legitimacy check completed. Results saved to:\", output_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgOkfqqspmna",
        "outputId": "d95340ec-8983-4d6c-cc20-f6aa8909fdbd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Message 1: Fake (Model: llama3-70b)\n",
            "Message 2: Fake (Model: llama3-70b)\n",
            "Message 3: Fake (Model: llama3-70b)\n",
            "Message 4: Fake (Model: llama3-70b)\n",
            "Message 5: Fake (Model: llama3-70b)\n",
            "Error during API request: Expecting value: line 1 column 1 (char 0)\n",
            "Error during API request: Expecting value: line 1 column 1 (char 0)\n",
            "Error during API request: Expecting value: line 1 column 1 (char 0)\n",
            "Error during API request: Expecting value: line 1 column 1 (char 0)\n",
            "Error during API request: Expecting value: line 1 column 1 (char 0)\n",
            "Message 6: Fake (Model: llama3-70b)\n",
            "Message 7: Fake. (Model: llama3-70b)\n",
            "Error during API request: Expecting value: line 1 column 1 (char 0)\n",
            "Message 8: Fake. (Model: llama3-70b)\n",
            "Message 9: Real (Model: llama3-70b)\n",
            "Message 10: Fake. (Model: llama3-70b)\n",
            "Message 11: Fake. (Model: llama3-70b)\n",
            "Error during API request: Expecting value: line 1 column 1 (char 0)\n",
            "Error during API request: Expecting value: line 1 column 1 (char 0)\n",
            "Message 12: Fake (Model: llama3-70b)\n",
            "Message 13: Fake (Model: llama3-70b)\n",
            "Message 14: Fake (Model: llama3-70b)\n",
            "Message 15: Fake (Model: llama3-70b)\n",
            "Message 16: Fake (Model: llama3-70b)\n",
            "Message 17: Fake (Model: llama3-70b)\n",
            "Message 18: Fake (Model: llama3-70b)\n",
            "Message 19: Fake (Model: llama3-70b)\n",
            "Message 20: Fake (Model: llama3-70b)\n",
            "Message 21: Fake (Model: llama3-70b)\n",
            "Error during API request: Expecting value: line 1 column 1 (char 0)\n",
            "Message 22: Fake (Model: llama3-70b)\n",
            "Message 23: Fake. (Model: llama3-70b)\n",
            "Error during API request: Expecting value: line 1 column 1 (char 0)\n",
            "Message 24: Fake (Model: llama3-70b)\n",
            "Message 25: Fake (Model: llama3-70b)\n",
            "Message 26: Fake (Model: llama3-70b)\n",
            "Message 27: Fake (Model: llama3-70b)\n",
            "Message 28: Fake (Model: llama3-70b)\n",
            "Message 29: Fake (Model: llama3-70b)\n",
            "Message 30: Fake (Model: llama3-70b)\n",
            "Message 31: Fake (Model: llama3-70b)\n",
            "Message 32: Fake. (Model: llama3-70b)\n",
            "Message 33: Fake (Model: llama3-70b)\n",
            "Error during API request: Expecting value: line 1 column 1 (char 0)\n",
            "Message 34: Fake (Model: llama3-70b)\n",
            "Message 35: Fake (Model: llama3-70b)\n",
            "Message 36: Fake (Model: llama3-70b)\n",
            "Message 37: Fake (Model: llama3-70b)\n",
            "Message 38: Fake (Model: llama3-70b)\n",
            "Message 39: Fake (Model: llama3-70b)\n",
            "Message 40: Fake (Model: llama3-70b)\n",
            "Legitimacy check completed. Results saved to: sms_legitimacy_results.xlsx\n"
          ]
        }
      ]
    }
  ]
}