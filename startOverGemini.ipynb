{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1l_FVQ5X5Hm_uGxMbP24XOh0fR6fffsnt",
      "authorship_tag": "ABX9TyOgo1oVYBT71r4pND2bcFbJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielhcg/AntiSmishGPT/blob/main/startOverGemini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "from google.colab import userdata\n",
        "import pandas as pd\n",
        "import time"
      ],
      "metadata": {
        "id": "iCZ0eSUPZPry"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = userdata.get('JOE-GEMINI-KEY')\n",
        "genai.configure(api_key=api_key)\n",
        "\n",
        "model = genai.GenerativeModel('models/gemini-1.5-pro')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Uvy1fWBhKNq9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ot-n5P-2Hys-",
        "outputId": "0469d08e-a291-4086-9c64-b09e28bca30a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Function to process a batch of messages\n",
        "def detect(messages):\n",
        "    prompts = [f\"Is this text message real or fake? '{message}' Respond with one word: real, fake, or undetermined, without any explanations.\" for message in messages]\n",
        "\n",
        "    while True:\n",
        "        response = model.generate_content(prompts)\n",
        "\n",
        "        # Extract text from the response and split\n",
        "        response_text = response.candidates[0].content.parts[0].text.strip()\n",
        "        split_results = response_text.split('\\n')\n",
        "\n",
        "        # Further filter results to ensure only valid responses are considered\n",
        "        valid_responses = ['real', 'fake', 'undetermined']\n",
        "        results = [line.strip().lower() for line in split_results if line.strip().lower() in valid_responses]\n",
        "\n",
        "        # Check if the results match the number of messages\n",
        "        if len(results) == len(messages):\n",
        "            return results\n",
        "        else:\n",
        "            print(f\"Retrying: Expected {len(messages)} results but got {len(results)}. Retrying...\")\n",
        "            time.sleep(60)  # Sleep for 1 minute before retrying\n",
        "\n",
        "# Function to process the CSV file\n",
        "def process_csv(input_file):\n",
        "    # Load the data into a pandas DataFrame without headers\n",
        "    df = pd.read_csv(input_file, header=None)\n",
        "\n",
        "    # To store results for each value\n",
        "    authenticity_results = []\n",
        "\n",
        "    # Determine the number of tokens per message\n",
        "    def count_tokens(text):\n",
        "        return len(text.split())\n",
        "\n",
        "    # Calculate tokens for each message\n",
        "    df['tokens'] = df[0].apply(count_tokens)\n",
        "\n",
        "    # Process rows in batches of 20 messages\n",
        "    batch_size = 20\n",
        "    max_requests_per_day = 50\n",
        "    max_tokens_per_minute = 32000\n",
        "    max_requests_per_minute = 2\n",
        "\n",
        "    requests_made = 0\n",
        "    total_tokens_processed = 0\n",
        "\n",
        "    for i in range(0, len(df), batch_size):\n",
        "        if requests_made >= max_requests_per_day:\n",
        "            print(\"Daily limit reached. Stopping execution.\")\n",
        "            break\n",
        "\n",
        "        batch = df[0][i:i+batch_size].tolist()\n",
        "        batch_tokens = df['tokens'][i:i+batch_size].sum()\n",
        "\n",
        "        if total_tokens_processed + batch_tokens > max_tokens_per_minute:\n",
        "            print(\"Token limit per minute reached. Waiting for the next minute.\")\n",
        "            time.sleep(60)  # Wait for the next minute\n",
        "            total_tokens_processed = 0  # Reset the token count for the new minute\n",
        "\n",
        "        try:\n",
        "            batch_results = detect(batch)\n",
        "        except Exception as e:\n",
        "            print(f\"Error during detection: {e}\")\n",
        "            batch_results = ['error'] * batch_size  # Fallback to error for all in case of error\n",
        "\n",
        "        authenticity_results.extend(batch_results)\n",
        "        total_tokens_processed += batch_tokens\n",
        "\n",
        "        # Print status after processing each batch\n",
        "        print(f\"Processed inputs {i + 1} to {i + batch_size}, Results = {batch_results}\")\n",
        "\n",
        "        # Rate limiting: ensure only 2 requests per minute\n",
        "        requests_made += 1\n",
        "        if requests_made % max_requests_per_minute == 0:\n",
        "            time.sleep(60)\n",
        "\n",
        "    # Ensure the authenticity results list is the same length as the DataFrame\n",
        "    if len(authenticity_results) < len(df):\n",
        "        authenticity_results.extend([''] * (len(df) - len(authenticity_results)))\n",
        "\n",
        "    # Assign results to new column\n",
        "    df['authenticity'] = authenticity_results\n",
        "\n",
        "    # Extract the directory from the input file path\n",
        "    input_directory = os.path.dirname(input_file)\n",
        "    output_file = os.path.join(input_directory, \"processed_\" + os.path.basename(input_file))\n",
        "\n",
        "    # Save the updated results back to the same directory\n",
        "    df.to_csv(output_file, index=False, header=False)\n",
        "\n",
        "    print(\"Processing complete. Results saved to\", output_file)\n",
        "\n",
        "# Example usage\n",
        "input_file = '/content/drive/My Drive/anti_smish_gpt/lastPart_smishTankSet.csv'  # Update with your file path\n",
        "process_csv(input_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "faXHlhx2_tmk",
        "outputId": "38d0ef8d-bedc-4004-b8e8-ce32062766fd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed inputs 1 to 20, Results = ['fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake']\n",
            "Processed inputs 21 to 40, Results = ['fake', 'fake', 'fake', 'fake', 'fake', 'undetermined', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake']\n",
            "Processed inputs 41 to 60, Results = ['fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'undetermined', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'undetermined', 'fake']\n",
            "Processed inputs 61 to 80, Results = ['fake', 'fake']\n",
            "Processing complete. Results saved to /content/drive/My Drive/anti_smish_gpt/processed_lastPart_smishTankSet.csv\n"
          ]
        }
      ]
    }
  ]
}