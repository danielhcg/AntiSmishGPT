{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNyI4asCa84ew721bQIrdS4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielhcg/AntiSmishGPT/blob/main/startOverGemini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "from google.colab import userdata\n",
        "import pandas as pd\n",
        "import time"
      ],
      "metadata": {
        "id": "iCZ0eSUPZPry"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=api_key)\n",
        "\n",
        "model = genai.GenerativeModel('models/gemini-1.5-pro')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Uvy1fWBhKNq9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# proof of concept to make sure api is working\n",
        "# only takes in a sinle message\n",
        "def detect(message):\n",
        "    prompt = f\"Is this text message real or fake? '{message}' Respond with one word: true, false, or undetermined.\"\n",
        "    response = model.generate_content(prompt)\n",
        "    return response.text.strip()\n",
        "\n",
        "# Example usage\n",
        "message = \"Your account has been suspended. Please click the link to reactivate it.\"\n",
        "result = detect(message)\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "Nd5DcAQjbiLa",
        "outputId": "68165a3b-35ad-4e92-c4fa-ce9403c79050"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Undetermined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# takes in entire spreadsheet and spits out new spreadsheet\n",
        "# doesn't consider rate limits\n",
        "# works for spreadsheet with one entry\n",
        "def detect(message):\n",
        "    prompt = f\"Is this text message real or fake? '{message}' Respond with one word: true, false, or undetermined.\"\n",
        "    response = model.generate_content(prompt)\n",
        "    return response.text.strip()\n",
        "\n",
        "# Define the input file path\n",
        "input_file = 'mendeley_set.xlsx'  # Update with your file path\n",
        "\n",
        "# Load the data into a pandas DataFrame without headers\n",
        "df = pd.read_excel(input_file, header=None)\n",
        "\n",
        "# Assume the messages are in the first column\n",
        "df['authenticity'] = df[0].apply(detect)\n",
        "\n",
        "# Save the updated results back to the same file\n",
        "df.to_excel(input_file, index=False, header=False)\n",
        "\n",
        "print(\"Processing complete. Results saved to\", input_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "xQqvwXXjb7Pf",
        "outputId": "28c3f058-1428-42d7-a5d3-a9c9712e2eaa"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing complete. Results saved to mendeley_set.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# takes in entire spreadsheet and spits out new spreadsheet\n",
        "# considers rate limit\n",
        "import time\n",
        "\n",
        "# Function to process a single message\n",
        "def detect(message):\n",
        "    prompt = f\"Is this text message real or fake? '{message}' Respond with one word: true, false, or undetermined.\"\n",
        "    response = model.generate_content(prompt)\n",
        "    return response.text.strip()\n",
        "\n",
        "# Function to process the Excel file\n",
        "def process_excel(input_file):\n",
        "    # Load the data into a pandas DataFrame without headers\n",
        "    df = pd.read_excel(input_file, header=None)\n",
        "\n",
        "    # To store results for each value\n",
        "    authenticity_results = []\n",
        "\n",
        "    # Determine the number of tokens per message\n",
        "    def count_tokens(text):\n",
        "        return len(text.split())\n",
        "\n",
        "    # Calculate tokens for each message\n",
        "    df['tokens'] = df[0].apply(count_tokens)\n",
        "\n",
        "    # Process rows in batches considering token limits\n",
        "    batch_size = 20  # Only one message per request to keep it simple\n",
        "    max_requests_per_day = 20\n",
        "    max_tokens_per_minute = 32000\n",
        "    max_requests_per_minute = 2\n",
        "\n",
        "    requests_made = 0\n",
        "    total_tokens_processed = 0\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        if requests_made >= max_requests_per_day:\n",
        "            print(\"Daily limit reached. Stopping execution.\")\n",
        "            break\n",
        "\n",
        "        message = df[0][i]\n",
        "        tokens = df['tokens'][i]\n",
        "\n",
        "        if total_tokens_processed + tokens > max_tokens_per_minute:\n",
        "            print(\"Token limit per minute reached. Waiting for the next minute.\")\n",
        "            time.sleep(60)  # Wait for the next minute\n",
        "            total_tokens_processed = 0  # Reset the token count for the new minute\n",
        "\n",
        "        result = detect(message)\n",
        "        authenticity_results.append(result)\n",
        "        total_tokens_processed += tokens\n",
        "\n",
        "        # Print status after processing each message\n",
        "        print(f\"Processed message {i + 1}/{len(df)}, Result = {result}\")\n",
        "\n",
        "        # Rate limiting: ensure only 2 requests per minute\n",
        "        requests_made += 1\n",
        "        if requests_made % max_requests_per_minute == 0:\n",
        "            time.sleep(60)\n",
        "\n",
        "    # Assign results to new column\n",
        "    df['authenticity'] = authenticity_results[:len(df)]\n",
        "\n",
        "    # Save the updated results back to the same file\n",
        "    output_file = \"processed_\" + input_file\n",
        "    df.to_excel(output_file, index=False, header=False)\n",
        "\n",
        "    print(\"Processing complete. Results saved to\", output_file)\n",
        "\n",
        "# Example usage\n",
        "input_file = 'mendeley_set.xlsx'  # Update with your file path\n",
        "process_excel(input_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "collapsed": true,
        "id": "222dl_XsoraF",
        "outputId": "a1f647a8-c3d5-46f6-800d-c7bdc325e760"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed message 1/40, Result = Undetermined\n",
            "Processed message 2/40, Result = Undetermined\n",
            "Processed message 3/40, Result = Undetermined\n",
            "Processed message 4/40, Result = Undetermined\n",
            "Processed message 5/40, Result = false\n",
            "Processed message 6/40, Result = false\n",
            "Processed message 7/40, Result = undetermined\n",
            "Processed message 8/40, Result = undetermined\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-0d5c48b34e42>\u001b[0m in \u001b[0;36m<cell line: 71>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;31m# Example usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0minput_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mendeley_set.xlsx'\u001b[0m  \u001b[0;31m# Update with your file path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0mprocess_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-34-0d5c48b34e42>\u001b[0m in \u001b[0;36mprocess_excel\u001b[0;34m(input_file)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mrequests_made\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrequests_made\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmax_requests_per_minute\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# Assign results to new column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to check what models are available\n",
        "import pprint\n",
        "for model in genai.list_models():\n",
        "    pprint.pprint(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "collapsed": true,
        "id": "6Luh0O-gPSRC",
        "outputId": "4677457b-058d-4467-e049-4fcb602d2b3f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(name='models/chat-bison-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='PaLM 2 Chat (Legacy)',\n",
            "      description='A legacy text-only model optimized for chat conversations',\n",
            "      input_token_limit=4096,\n",
            "      output_token_limit=1024,\n",
            "      supported_generation_methods=['generateMessage', 'countMessageTokens'],\n",
            "      temperature=0.25,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/text-bison-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='PaLM 2 (Legacy)',\n",
            "      description='A legacy model that understands text and generates text as an output',\n",
            "      input_token_limit=8196,\n",
            "      output_token_limit=1024,\n",
            "      supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
            "      temperature=0.7,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/embedding-gecko-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Embedding Gecko',\n",
            "      description='Obtain a distributed representation of a text.',\n",
            "      input_token_limit=1024,\n",
            "      output_token_limit=1,\n",
            "      supported_generation_methods=['embedText', 'countTextTokens'],\n",
            "      temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-1.0-pro',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro',\n",
            "      description='The best model for scaling across a wide range of tasks',\n",
            "      input_token_limit=30720,\n",
            "      output_token_limit=2048,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.9,\n",
            "      top_p=1.0,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-1.0-pro-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro 001 (Tuning)',\n",
            "      description=('The best model for scaling across a wide range of tasks. This is a stable '\n",
            "                   'model that supports tuning.'),\n",
            "      input_token_limit=30720,\n",
            "      output_token_limit=2048,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
            "      temperature=0.9,\n",
            "      top_p=1.0,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-1.0-pro-latest',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro Latest',\n",
            "      description=('The best model for scaling across a wide range of tasks. This is the latest '\n",
            "                   'model.'),\n",
            "      input_token_limit=30720,\n",
            "      output_token_limit=2048,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.9,\n",
            "      top_p=1.0,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-1.0-pro-vision-latest',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro Vision',\n",
            "      description='The best image understanding model to handle a broad range of applications',\n",
            "      input_token_limit=12288,\n",
            "      output_token_limit=4096,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.4,\n",
            "      top_p=1.0,\n",
            "      top_k=32)\n",
            "Model(name='models/gemini-1.5-flash',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash',\n",
            "      description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-1.5-flash-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash 001',\n",
            "      description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
            "      temperature=1.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-1.5-flash-latest',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash Latest',\n",
            "      description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-1.5-pro',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Pro',\n",
            "      description='Mid-size multimodal model that supports up to 1 million tokens',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-1.5-pro-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Pro 001',\n",
            "      description='Mid-size multimodal model that supports up to 1 million tokens',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
            "      temperature=1.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-1.5-pro-latest',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Pro Latest',\n",
            "      description='Mid-size multimodal model that supports up to 1 million tokens',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-pro',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro',\n",
            "      description='The best model for scaling across a wide range of tasks',\n",
            "      input_token_limit=30720,\n",
            "      output_token_limit=2048,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.9,\n",
            "      top_p=1.0,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-pro-vision',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro Vision',\n",
            "      description='The best image understanding model to handle a broad range of applications',\n",
            "      input_token_limit=12288,\n",
            "      output_token_limit=4096,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.4,\n",
            "      top_p=1.0,\n",
            "      top_k=32)\n",
            "Model(name='models/embedding-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Embedding 001',\n",
            "      description='Obtain a distributed representation of a text.',\n",
            "      input_token_limit=2048,\n",
            "      output_token_limit=1,\n",
            "      supported_generation_methods=['embedContent'],\n",
            "      temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/text-embedding-004',\n",
            "      base_model_id='',\n",
            "      version='004',\n",
            "      display_name='Text Embedding 004',\n",
            "      description='Obtain a distributed representation of a text.',\n",
            "      input_token_limit=2048,\n",
            "      output_token_limit=1,\n",
            "      supported_generation_methods=['embedContent'],\n",
            "      temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/aqa',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Model that performs Attributed Question Answering.',\n",
            "      description=('Model trained to return answers to questions that are grounded in provided '\n",
            "                   'sources, along with estimating answerable probability.'),\n",
            "      input_token_limit=7168,\n",
            "      output_token_limit=1024,\n",
            "      supported_generation_methods=['generateAnswer'],\n",
            "      temperature=0.2,\n",
            "      top_p=1.0,\n",
            "      top_k=40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#processed sheet, but output was incorrect\n",
        "\n",
        "# Function to process a batch of messages\n",
        "def detect(messages):\n",
        "    prompts = [f\"Is this text message real or fake? '{message}' Respond with one word: true, false, or undetermined.\" for message in messages]\n",
        "    responses = model.generate_content(prompts)  # Assuming the model can handle a list of prompts\n",
        "    return [response.text.strip() for response in responses]\n",
        "\n",
        "# Function to process the Excel file\n",
        "def process_excel(input_file):\n",
        "    # Load the data into a pandas DataFrame without headers\n",
        "    df = pd.read_excel(input_file, header=None)\n",
        "\n",
        "    # To store results for each value\n",
        "    authenticity_results = []\n",
        "\n",
        "    # Determine the number of tokens per message\n",
        "    def count_tokens(text):\n",
        "        return len(text.split())\n",
        "\n",
        "    # Calculate tokens for each message\n",
        "    df['tokens'] = df[0].apply(count_tokens)\n",
        "\n",
        "    # Process rows in batches of 20 messages\n",
        "    batch_size = 20\n",
        "    max_requests_per_day = 50\n",
        "    max_tokens_per_minute = 32000\n",
        "    max_requests_per_minute = 2\n",
        "\n",
        "    requests_made = 0\n",
        "    total_tokens_processed = 0\n",
        "\n",
        "    for i in range(0, len(df), batch_size):\n",
        "        if requests_made >= max_requests_per_day:\n",
        "            print(\"Daily limit reached. Stopping execution.\")\n",
        "            break\n",
        "\n",
        "        batch = df[0][i:i+batch_size].tolist()\n",
        "        batch_tokens = df['tokens'][i:i+batch_size].sum()\n",
        "\n",
        "        if total_tokens_processed + batch_tokens > max_tokens_per_minute:\n",
        "            print(\"Token limit per minute reached. Waiting for the next minute.\")\n",
        "            time.sleep(60)  # Wait for the next minute\n",
        "            total_tokens_processed = 0  # Reset the token count for the new minute\n",
        "\n",
        "        batch_results = detect(batch)\n",
        "        authenticity_results.extend(batch_results)\n",
        "        total_tokens_processed += batch_tokens\n",
        "\n",
        "        # Print status after processing each batch\n",
        "        print(f\"Processed inputs {i + 1} to {i + len(batch)}, Results = {batch_results}\")\n",
        "\n",
        "        # Rate limiting: ensure only 2 requests per minute\n",
        "        requests_made += 1\n",
        "        if requests_made % max_requests_per_minute == 0:\n",
        "            time.sleep(60)\n",
        "\n",
        "    # Assign results to new column\n",
        "    df['authenticity'] = pd.Series(authenticity_results)\n",
        "\n",
        "    # Save the updated results back to the same file\n",
        "    output_file = \"processed_\" + input_file\n",
        "    df.to_excel(output_file, index=False, header=False)\n",
        "\n",
        "    print(\"Processing complete. Results saved to\", output_file)\n",
        "\n",
        "# Example usage\n",
        "input_file = 'mendeley_set.xlsx'  # Update with your file path\n",
        "process_excel(input_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "fV3VAB79tF7g",
        "outputId": "eab2ec56-ba27-4be4-b8a6-3bd7d76c1aed"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed inputs 1 to 20, Results = ['Undetermined\\nUndetermined\\nUndetermined\\nUndetermined\\nFalse\\nFalse\\nUndetermined\\nUndetermined\\nUndetermined\\nUndetermined\\nUndetermined\\nUndetermined\\nUndetermined\\nUndetermined\\nUndetermined\\nFalse\\nUndetermined\\nTrue\\nUndetermined\\nUndetermined']\n",
            "Processed inputs 21 to 40, Results = [\"Here's a breakdown of each text message and whether they seem real or fake:\\n\\n* **'(That said can you text him one more time?)'** - **Undetermined** (Could be real, depends on context)\\n* **'I know she called me'** - **Undetermined** (Could be real, depends on context) \\n* **'In sch but neva mind u eat 1st lor..'** - **Undetermined** (Uses slang/shortening, could be real)\\n* **'lyricalladie(21/F) is inviting you to be her friend. Reply YES-910 or NO-910. See her: www.SMS.ac/u/hmmross STOP? Send STOP FRND to 62468'** - **False** (Classic spam format)\\n* **'Jay says that you're a double-faggot'** - **Undetermined** (Could be real, but uses offensive language)\\n* **'Ok i thk i got it. Then u wan me 2 come now or wat?'** -  **Undetermined** (Uses slang/shortening, could be real)\\n* **'UR awarded a City Break and could WIN a Â£200 Summer Shopping spree every WK. Txt STORE to 88039 . SkilGme. TsCs087147403233'** - **False** (Classic spam format with suspicious number)\\n* **'Hello from Orange. For 1 month's free access to games, news and sport, plus 10 free texts and 20 photo messages, reply YES. Terms apply: www.orange.co.uk/ow'** - **Undetermined** (Could be real, but needs verification of the offer)\\n* **'Oh... Lk tt den we take e one tt ends at cine lor... Dun wan yogasana oso can...'** - **Undetermined** (Uses slang/shortening, difficult to determine)\\n* **'Whens your radio show?'** - **Undetermined** (Could be real, depends on context)\\n* **'Jane babes not goin 2 wrk, feel ill after lst nite. Foned in already cover 4 me chuck.:-)'** - **Undetermined** (Uses slang/shortening, could be real)\\n* **'New Offer! Save upto 40\\\\% electricity bill with Power Saver(GOVT. LAB TESTED), Rs. 1050/-(free home delivery) 3 Yr. Guarantee Call 9891943823,9891943780'** - **False** (Likely a scam, too good to be true offer)\\n* **'What * u wearing?'** - **Undetermined** (Could be real, but could be inappropriate depending on context)\\n* **'o turns out i had stereo love on mi phone under the unknown album.'** - **Undetermined** (Uses slang/shortening, could be real)\\n* **'Ela kano.,il download, come wen ur free..'** - **Undetermined** (Mix of language/slang, difficult to determine)\\n* **'Haha figures, well I found the piece and priscilla's bowl'** - **Undetermined** (Could be real, depends on context) \\n* **'Can you do online transaction?'** - **Undetermined** (Could be real, depends on context)\\n* **'Ãœ neva tell me how i noe... I'm not at home in da aft wat...'** - **Undetermined** (Uses slang/shortening, difficult to determine)\\n* **'YOU HAVE WON! As a valued Vodafone customer our computer has picked YOU to win a Â£150 prize. To collect is easy. Just call 09061743386'** - **False** (Classic scam format with suspicious number) \\n* **'You will recieve your tone within the next 24hrs. For Terms and conditions please see Channel U Teletext Pg 750'** - **Undetermined** (Could be real, but from an older system of delivering content)\"]\n",
            "Processing complete. Results saved to processed_mendeley_set.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#most up to date\n",
        "#handles everything\n",
        "\n",
        "# Function to process a batch of messages\n",
        "def detect(messages):\n",
        "    prompts = [f\"Is this text message real or fake? '{message}' Respond with one word: Real, Fake, or Undetermined, without any explanations.\" for message in messages]\n",
        "    response = model.generate_content(prompts)\n",
        "\n",
        "    # Debugging: Print response object to understand its structure\n",
        "    print(f\"Response: {response}\")\n",
        "\n",
        "    # Extract text from the response and split\n",
        "    response_text = response.candidates[0].content.parts[0].text.strip()\n",
        "    split_results = response_text.split('\\n')\n",
        "\n",
        "    # Further filter results to ensure only valid responses are considered\n",
        "    valid_responses = ['true', 'false', 'undetermined']\n",
        "    results = [line.strip().lower() for line in split_results if line.strip().lower() in valid_responses]\n",
        "\n",
        "    # Debugging: Print filtered results\n",
        "    print(f\"Filtered results: {results}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Function to process the CSV file\n",
        "def process_csv(input_file):\n",
        "    # Load the data into a pandas DataFrame without headers\n",
        "    df = pd.read_csv(input_file, header=None)\n",
        "\n",
        "    # To store results for each value\n",
        "    authenticity_results = []\n",
        "\n",
        "    # Determine the number of tokens per message\n",
        "    def count_tokens(text):\n",
        "        return len(text.split())\n",
        "\n",
        "    # Calculate tokens for each message\n",
        "    df['tokens'] = df[0].apply(count_tokens)\n",
        "\n",
        "    # Process rows in batches of 20 messages\n",
        "    batch_size = 20\n",
        "    max_requests_per_day = 50\n",
        "    max_tokens_per_minute = 32000\n",
        "    max_requests_per_minute = 2\n",
        "\n",
        "    requests_made = 0\n",
        "    total_tokens_processed = 0\n",
        "\n",
        "    for i in range(0, len(df), batch_size):\n",
        "        if requests_made >= max_requests_per_day:\n",
        "            print(\"Daily limit reached. Stopping execution.\")\n",
        "            break\n",
        "\n",
        "        batch = df[0][i:i+batch_size].tolist()\n",
        "        batch_tokens = df['tokens'][i:i+batch_size].sum()\n",
        "\n",
        "        if total_tokens_processed + batch_tokens > max_tokens_per_minute:\n",
        "            print(\"Token limit per minute reached. Waiting for the next minute.\")\n",
        "            time.sleep(60)  # Wait for the next minute\n",
        "            total_tokens_processed = 0  # Reset the token count for the new minute\n",
        "\n",
        "        try:\n",
        "            batch_results = detect(batch)\n",
        "        except Exception as e:\n",
        "            print(f\"Error during detection: {e}\")\n",
        "            batch_results = ['undetermined'] * batch_size  # Fallback to undetermined for all in case of error\n",
        "\n",
        "        if len(batch_results) != batch_size:\n",
        "            print(f\"Error: Expected {batch_size} results but got {len(batch_results)}. Continuing with next batch.\")\n",
        "            batch_results += ['undetermined'] * (batch_size - len(batch_results))  # Pad the results\n",
        "\n",
        "        authenticity_results.extend(batch_results)\n",
        "        total_tokens_processed += batch_tokens\n",
        "\n",
        "        # Print status after processing each batch\n",
        "        print(f\"Processed inputs {i + 1} to {i + batch_size}, Results = {batch_results}\")\n",
        "\n",
        "        # Rate limiting: ensure only 2 requests per minute\n",
        "        requests_made += 1\n",
        "        if requests_made % max_requests_per_minute == 0:\n",
        "            time.sleep(60)\n",
        "\n",
        "    # Ensure the authenticity results list is the same length as the DataFrame\n",
        "    if len(authenticity_results) < len(df):\n",
        "        authenticity_results.extend([''] * (len(df) - len(authenticity_results)))\n",
        "\n",
        "    # Assign results to new column\n",
        "    df['authenticity'] = authenticity_results\n",
        "\n",
        "    # Save the updated results back to the same file\n",
        "    output_file = \"processed_\" + input_file\n",
        "    df.to_csv(output_file, index=False, header=False)\n",
        "\n",
        "    print(\"Processing complete. Results saved to\", output_file)\n",
        "\n",
        "# Example usage\n",
        "input_file = 'mendeley_set.csv'  # Update with your file path\n",
        "process_csv(input_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "51X46dmFyQRi",
        "outputId": "bf179bdd-52f9-42f1-c529-30253bdd8279"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=glm.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Undetermined\\nUndetermined\\nFake\\nUndetermined\\nFake\\nFake\\nReal\\nReal\\nReal\\nReal\\nReal\\nUndetermined\\nReal\\nReal\\nUndetermined\\nFake\\nReal\\nReal\\nReal\\nFake\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": 1,\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ],\n",
            "          \"token_count\": 0,\n",
            "          \"grounding_attributions\": []\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 968,\n",
            "        \"candidates_token_count\": 44,\n",
            "        \"total_token_count\": 1012\n",
            "      }\n",
            "    }),\n",
            ")\n",
            "Filtered results: ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Error: Expected 20 results but got 5. Continuing with next batch.\n",
            "Processed inputs 1 to 20, Results = ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Response: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=glm.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Real\\nReal\\nFake\\nFake\\nFake\\nReal\\nFake\\nReal\\nFake\\nReal\\nReal\\nFake\\nReal\\nFake\\nReal\\nReal\\nReal\\nFake\\nFake \\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": 1,\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ],\n",
            "          \"token_count\": 0,\n",
            "          \"grounding_attributions\": []\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 1004,\n",
            "        \"candidates_token_count\": 37,\n",
            "        \"total_token_count\": 1041\n",
            "      }\n",
            "    }),\n",
            ")\n",
            "Filtered results: []\n",
            "Error: Expected 20 results but got 0. Continuing with next batch.\n",
            "Processed inputs 21 to 40, Results = ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Response: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=glm.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Undetermined\\nReal\\nFake\\nUndetermined\\nUndetermined\\nUndetermined\\nReal\\nFake\\nFake\\nUndetermined\\nReal\\nReal\\nReal\\nReal\\nFake\\nReal\\nUndetermined\\nReal\\nReal\\nReal \\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": 1,\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ],\n",
            "          \"token_count\": 0,\n",
            "          \"grounding_attributions\": []\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 1011,\n",
            "        \"candidates_token_count\": 45,\n",
            "        \"total_token_count\": 1056\n",
            "      }\n",
            "    }),\n",
            ")\n",
            "Filtered results: ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Error: Expected 20 results but got 6. Continuing with next batch.\n",
            "Processed inputs 41 to 60, Results = ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Response: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=glm.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Undetermined\\nReal\\nFake\\nReal\\nReal\\nUndetermined\\nReal\\nFake\\nReal\\nReal\\nFake\\nReal\\nReal\\nFake\\nReal\\nReal\\nReal\\nReal\\nReal\\nReal \\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": 1,\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ],\n",
            "          \"token_count\": 0,\n",
            "          \"grounding_attributions\": []\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 1139,\n",
            "        \"candidates_token_count\": 41,\n",
            "        \"total_token_count\": 1180\n",
            "      }\n",
            "    }),\n",
            ")\n",
            "Filtered results: ['undetermined', 'undetermined']\n",
            "Error: Expected 20 results but got 2. Continuing with next batch.\n",
            "Processed inputs 61 to 80, Results = ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Response: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=glm.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Real\\nReal\\nReal\\nFake\\nReal\\nReal\\nFake\\nReal\\nReal\\nFake\\nReal\\nReal\\nFake\\nReal\\nReal\\nReal\\nReal\\nFake\\nReal\\nReal \\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": 1,\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 2,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ],\n",
            "          \"token_count\": 0,\n",
            "          \"grounding_attributions\": []\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 1107,\n",
            "        \"candidates_token_count\": 39,\n",
            "        \"total_token_count\": 1146\n",
            "      }\n",
            "    }),\n",
            ")\n",
            "Filtered results: []\n",
            "Error: Expected 20 results but got 0. Continuing with next batch.\n",
            "Processed inputs 81 to 100, Results = ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Response: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=glm.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Real\\nUndetermined\\nReal\\nReal\\nFake\\nFake\\nReal\\nReal\\nReal\\nFake\\nReal\\nReal\\nReal\\nReal\\nFake\\nReal\\nFake\\nFake\\nReal\\nReal \\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": 1,\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ],\n",
            "          \"token_count\": 0,\n",
            "          \"grounding_attributions\": []\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 983,\n",
            "        \"candidates_token_count\": 40,\n",
            "        \"total_token_count\": 1023\n",
            "      }\n",
            "    }),\n",
            ")\n",
            "Filtered results: ['undetermined']\n",
            "Error: Expected 20 results but got 1. Continuing with next batch.\n",
            "Processed inputs 101 to 120, Results = ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Response: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=glm.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Fake\\nFake\\nUndetermined\\nReal\\nReal\\nFake\\nReal\\nReal\\nUndetermined\\nReal\\nReal\\nFake\\nReal\\nReal\\nReal\\nReal\\nReal\\nReal\\nReal\\nReal \\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": 1,\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ],\n",
            "          \"token_count\": 0,\n",
            "          \"grounding_attributions\": []\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 1011,\n",
            "        \"candidates_token_count\": 41,\n",
            "        \"total_token_count\": 1052\n",
            "      }\n",
            "    }),\n",
            ")\n",
            "Filtered results: ['undetermined', 'undetermined']\n",
            "Error: Expected 20 results but got 2. Continuing with next batch.\n",
            "Processed inputs 121 to 140, Results = ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Response: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=glm.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Fake\\nReal\\nFake\\nReal\\nFake\\nReal\\nReal\\nUndetermined\\nReal\\nReal\\nFake\\nReal\\nReal\\nFake\\nReal\\nFake\\nFake\\nReal\\nReal\\nReal \\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": 1,\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ],\n",
            "          \"token_count\": 0,\n",
            "          \"grounding_attributions\": []\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 1009,\n",
            "        \"candidates_token_count\": 40,\n",
            "        \"total_token_count\": 1049\n",
            "      }\n",
            "    }),\n",
            ")\n",
            "Filtered results: ['undetermined']\n",
            "Error: Expected 20 results but got 1. Continuing with next batch.\n",
            "Processed inputs 141 to 160, Results = ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Response: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=glm.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Fake\\nFake\\nReal\\nFake\\nReal\\nFake\\nFake\\nFake\\nReal\\nFake\\nReal\\nFake\\nReal\\nFake\\nReal\\nReal\\nReal\\nReal\\nReal\\nReal \\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": 1,\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 2,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ],\n",
            "          \"token_count\": 0,\n",
            "          \"grounding_attributions\": []\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 1019,\n",
            "        \"candidates_token_count\": 39,\n",
            "        \"total_token_count\": 1058\n",
            "      }\n",
            "    }),\n",
            ")\n",
            "Filtered results: []\n",
            "Error: Expected 20 results but got 0. Continuing with next batch.\n",
            "Processed inputs 161 to 180, Results = ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Response: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=glm.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Fake\\nFake\\nUndetermined\\nFake\\nUndetermined\\nUndetermined\\nFake\\nFake\\nUndetermined\\nFake\\nUndetermined\\nUndetermined\\nReal\\nFake\\nUndetermined\\nFake\\nFake\\nUndetermined\\nUndetermined\\nFake \\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": 1,\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ],\n",
            "          \"token_count\": 0,\n",
            "          \"grounding_attributions\": []\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 1080,\n",
            "        \"candidates_token_count\": 48,\n",
            "        \"total_token_count\": 1128\n",
            "      }\n",
            "    }),\n",
            ")\n",
            "Filtered results: ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Error: Expected 20 results but got 9. Continuing with next batch.\n",
            "Processed inputs 181 to 200, Results = ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Response: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=glm.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Undetermined\\nFake\\nReal\\nFake\\nUndetermined\\nUndetermined\\nUndetermined\\nFake\\nUndetermined\\nUndetermined\\nReal\\nReal\\nUndetermined\\nFake\\nReal\\nReal\\nReal\\nUndetermined\\nReal\\nReal \\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": 1,\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ],\n",
            "          \"token_count\": 0,\n",
            "          \"grounding_attributions\": []\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 1005,\n",
            "        \"candidates_token_count\": 47,\n",
            "        \"total_token_count\": 1052\n",
            "      }\n",
            "    }),\n",
            ")\n",
            "Filtered results: ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Error: Expected 20 results but got 8. Continuing with next batch.\n",
            "Processed inputs 201 to 220, Results = ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Response: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=glm.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Undetermined\\nReal\\nFake\\nFake\\nReal\\nReal\\nFake\\nReal\\nFake\\nReal\\nReal\\nFake\\nReal\\nFake\\nFake\\nReal\\nReal\\nReal\\nReal\\nReal \\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": 1,\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ],\n",
            "          \"token_count\": 0,\n",
            "          \"grounding_attributions\": []\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 1094,\n",
            "        \"candidates_token_count\": 40,\n",
            "        \"total_token_count\": 1134\n",
            "      }\n",
            "    }),\n",
            ")\n",
            "Filtered results: ['undetermined']\n",
            "Error: Expected 20 results but got 1. Continuing with next batch.\n",
            "Processed inputs 221 to 240, Results = ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Response: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=glm.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Real\\nFake\\nReal\\nReal\\nReal\\nReal\\nFake\\nReal\\nReal\\nReal\\nReal\\nReal\\nReal\\nFake\\nFake\\nFake\\nReal\\nReal\\nFake\\nReal \\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": 1,\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ],\n",
            "          \"token_count\": 0,\n",
            "          \"grounding_attributions\": []\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 1044,\n",
            "        \"candidates_token_count\": 39,\n",
            "        \"total_token_count\": 1083\n",
            "      }\n",
            "    }),\n",
            ")\n",
            "Filtered results: []\n",
            "Error: Expected 20 results but got 0. Continuing with next batch.\n",
            "Processed inputs 241 to 260, Results = ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Response: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=glm.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Real\\nReal\\nReal\\nReal\\nReal\\nReal\\nReal\\nReal\\nReal\\nReal\\nFake\\nFake\\nReal\\nReal\\nReal\\nReal\\nReal\\nReal\\nReal\\nReal \\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": 1,\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 2,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ],\n",
            "          \"token_count\": 0,\n",
            "          \"grounding_attributions\": []\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 966,\n",
            "        \"candidates_token_count\": 39,\n",
            "        \"total_token_count\": 1005\n",
            "      }\n",
            "    }),\n",
            ")\n",
            "Filtered results: []\n",
            "Error: Expected 20 results but got 0. Continuing with next batch.\n",
            "Processed inputs 261 to 280, Results = ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Response: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=glm.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Real\\nFake\\nFake\\nReal\\nFake\\nReal\\nReal\\nReal\\nReal\\nReal\\nReal\\nReal\\nReal\\nFake\\nReal\\nReal\\nReal\\nFake\\nReal\\nReal \\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": 1,\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ],\n",
            "          \"token_count\": 0,\n",
            "          \"grounding_attributions\": []\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 967,\n",
            "        \"candidates_token_count\": 39,\n",
            "        \"total_token_count\": 1006\n",
            "      }\n",
            "    }),\n",
            ")\n",
            "Filtered results: []\n",
            "Error: Expected 20 results but got 0. Continuing with next batch.\n",
            "Processed inputs 281 to 300, Results = ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Response: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=glm.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Undetermined\\nFake\\nReal\\nUndetermined\\nUndetermined\\nFake\\nReal\\nFake\\nReal\\nReal\\nReal\\nUndetermined\\nReal\\nReal\\nUndetermined\\nFake\\nReal\\nReal\\nFake\\nReal \\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": 1,\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ],\n",
            "          \"token_count\": 0,\n",
            "          \"grounding_attributions\": []\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 1030,\n",
            "        \"candidates_token_count\": 44,\n",
            "        \"total_token_count\": 1074\n",
            "      }\n",
            "    }),\n",
            ")\n",
            "Filtered results: ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Error: Expected 20 results but got 5. Continuing with next batch.\n",
            "Processed inputs 301 to 320, Results = ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Response: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=glm.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Undetermined\\nFake\\nFake\\nReal\\nUndetermined\\nReal\\nReal\\nUndetermined\\nUndetermined\\nReal\\nUndetermined\\nReal\\nFake\\nFake\\nFake\\nReal\\nReal\\nReal\\nUndetermined\\nReal \\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": 1,\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ],\n",
            "          \"token_count\": 0,\n",
            "          \"grounding_attributions\": []\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 937,\n",
            "        \"candidates_token_count\": 45,\n",
            "        \"total_token_count\": 982\n",
            "      }\n",
            "    }),\n",
            ")\n",
            "Filtered results: ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Error: Expected 20 results but got 6. Continuing with next batch.\n",
            "Processed inputs 321 to 340, Results = ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Response: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=glm.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Real\\nReal\\nReal\\nReal\\nReal\\nReal\\nFake\\nReal\\nReal\\nReal\\nReal\\nReal\\nFake\\nFake\\nReal\\nReal\\nReal\\nReal\\nFake\\nReal \\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": 1,\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ],\n",
            "          \"token_count\": 0,\n",
            "          \"grounding_attributions\": []\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 1069,\n",
            "        \"candidates_token_count\": 39,\n",
            "        \"total_token_count\": 1108\n",
            "      }\n",
            "    }),\n",
            ")\n",
            "Filtered results: []\n",
            "Error: Expected 20 results but got 0. Continuing with next batch.\n",
            "Processed inputs 341 to 360, Results = ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Response: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=glm.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Real\\nReal\\nReal\\nFake\\nReal\\nReal\\nFake\\nReal\\nFake\\nReal\\nReal\\nFake\\nReal\\nReal\\nReal\\nReal\\nReal\\nReal\\nFake\\nFake \\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": 1,\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ],\n",
            "          \"token_count\": 0,\n",
            "          \"grounding_attributions\": []\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 1039,\n",
            "        \"candidates_token_count\": 39,\n",
            "        \"total_token_count\": 1078\n",
            "      }\n",
            "    }),\n",
            ")\n",
            "Filtered results: []\n",
            "Error: Expected 20 results but got 0. Continuing with next batch.\n",
            "Processed inputs 361 to 380, Results = ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Response: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=glm.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Fake\\nFake\\nFake\\nUndetermined\\nFake\\nReal\\nReal\\nReal\\nFake\\nUndetermined\\nReal\\nFake\\nReal\\nReal\\nReal\\nReal\\nReal\\nReal\\nFake\\nFake \\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": 1,\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ],\n",
            "          \"token_count\": 0,\n",
            "          \"grounding_attributions\": []\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 1133,\n",
            "        \"candidates_token_count\": 41,\n",
            "        \"total_token_count\": 1174\n",
            "      }\n",
            "    }),\n",
            ")\n",
            "Filtered results: ['undetermined', 'undetermined']\n",
            "Error: Expected 20 results but got 2. Continuing with next batch.\n",
            "Processed inputs 381 to 400, Results = ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Response: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=glm.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Real\\nFake\\nUndetermined\\nReal\\nReal\\nReal\\nReal\\nReal\\nReal\\nFake\\nReal\\nReal\\nFake\\nReal\\nFake\\nReal\\nReal\\nReal\\nReal\\nReal \\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": 1,\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ],\n",
            "          \"token_count\": 0,\n",
            "          \"grounding_attributions\": []\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 892,\n",
            "        \"candidates_token_count\": 40,\n",
            "        \"total_token_count\": 932\n",
            "      }\n",
            "    }),\n",
            ")\n",
            "Filtered results: ['undetermined']\n",
            "Error: Expected 20 results but got 1. Continuing with next batch.\n",
            "Processed inputs 401 to 420, Results = ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Response: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=glm.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Undetermined\\nUndetermined\\nFake\\nUndetermined\\nFake\\nReal\\nFake\\nFake\\nReal\\nReal\\nReal\\nFake\\nFake\\nReal\\nFake\\nReal\\nFake\\nReal\\nReal\\nReal \\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": 1,\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ],\n",
            "          \"token_count\": 0,\n",
            "          \"grounding_attributions\": []\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 1006,\n",
            "        \"candidates_token_count\": 42,\n",
            "        \"total_token_count\": 1048\n",
            "      }\n",
            "    }),\n",
            ")\n",
            "Filtered results: ['undetermined', 'undetermined', 'undetermined']\n",
            "Error: Expected 20 results but got 3. Continuing with next batch.\n",
            "Processed inputs 421 to 440, Results = ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Response: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=glm.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Real\\nFake\\nReal\\nFake\\nFake\\nReal\\nReal\\nReal\\nFake\\nReal\\nReal\\nFake\\nReal\\nReal\\nReal\\nReal\\nFake\\nReal\\nReal\\nFake \\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": 1,\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ],\n",
            "          \"token_count\": 0,\n",
            "          \"grounding_attributions\": []\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 930,\n",
            "        \"candidates_token_count\": 39,\n",
            "        \"total_token_count\": 969\n",
            "      }\n",
            "    }),\n",
            ")\n",
            "Filtered results: []\n",
            "Error: Expected 20 results but got 0. Continuing with next batch.\n",
            "Processed inputs 441 to 460, Results = ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Response: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=glm.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Fake\\nReal\\nReal\\nReal\\nUndetermined\\nFake\\nFake\\nUndetermined\\nReal\\nFake\\nFake\\nReal\\nFake\\nReal\\nFake\\nReal\\nReal\\nFake\\nReal\\nUndetermined \\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": 1,\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 2,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ],\n",
            "          \"token_count\": 0,\n",
            "          \"grounding_attributions\": []\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 978,\n",
            "        \"candidates_token_count\": 42,\n",
            "        \"total_token_count\": 1020\n",
            "      }\n",
            "    }),\n",
            ")\n",
            "Filtered results: ['undetermined', 'undetermined', 'undetermined']\n",
            "Error: Expected 20 results but got 3. Continuing with next batch.\n",
            "Processed inputs 461 to 480, Results = ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Response: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=glm.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Real\\nFake\\nReal\\nFake\\nFake\\nReal\\nFake\\nReal\\nReal\\nFake\\nReal\\nReal\\nReal\\nUndetermined\\nReal\\nReal\\nReal\\nFake\\nFake\\nReal \\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": 1,\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 2,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 2,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ],\n",
            "          \"token_count\": 0,\n",
            "          \"grounding_attributions\": []\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 1080,\n",
            "        \"candidates_token_count\": 40,\n",
            "        \"total_token_count\": 1120\n",
            "      }\n",
            "    }),\n",
            ")\n",
            "Filtered results: ['undetermined']\n",
            "Error: Expected 20 results but got 1. Continuing with next batch.\n",
            "Processed inputs 481 to 500, Results = ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Response: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=glm.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Fake\\nUndetermined\\nFake\\nFake\\nReal\\nReal\\nFake\\nReal\\nReal\\nFake\\nReal\\nReal\\nReal\\nReal\\nFake\\nFake\\nFake\\nReal\\nReal\\nFake \\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": 1,\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ],\n",
            "          \"token_count\": 0,\n",
            "          \"grounding_attributions\": []\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 1207,\n",
            "        \"candidates_token_count\": 40,\n",
            "        \"total_token_count\": 1247\n",
            "      }\n",
            "    }),\n",
            ")\n",
            "Filtered results: ['undetermined']\n",
            "Error: Expected 20 results but got 1. Continuing with next batch.\n",
            "Processed inputs 501 to 520, Results = ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Response: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=glm.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Fake\\nReal\\nReal\\nReal\\nReal\\nReal\\nReal\\nReal\\nReal\\nReal\\nReal\\nReal\\nReal\\nFake\\nReal\\nReal\\nReal\\nReal\\nReal\\nReal \\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": 1,\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ],\n",
            "          \"token_count\": 0,\n",
            "          \"grounding_attributions\": []\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 975,\n",
            "        \"candidates_token_count\": 39,\n",
            "        \"total_token_count\": 1014\n",
            "      }\n",
            "    }),\n",
            ")\n",
            "Filtered results: []\n",
            "Error: Expected 20 results but got 0. Continuing with next batch.\n",
            "Processed inputs 521 to 540, Results = ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Response: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=glm.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Undetermined\\nReal\\nReal\\nFake\\nReal\\nReal\\nFake\\nFake\\nFake\\nFake\\nFake\\nReal\\nFake\\nFake\\nFake\\nReal\\nFake\\nReal\\nReal\\nReal \\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": 1,\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ],\n",
            "          \"token_count\": 0,\n",
            "          \"grounding_attributions\": []\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 1075,\n",
            "        \"candidates_token_count\": 40,\n",
            "        \"total_token_count\": 1115\n",
            "      }\n",
            "    }),\n",
            ")\n",
            "Filtered results: ['undetermined']\n",
            "Error: Expected 20 results but got 1. Continuing with next batch.\n",
            "Processed inputs 541 to 560, Results = ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Response: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=glm.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Undetermined\\nReal\\nFake\\nUndetermined\\nFake\\nFake\\nReal\\nFake\\nReal\\nUndetermined\\nFake\\nFake\\nFake\\nReal\\nFake\\nFake\\nUndetermined\\nUndetermined\\nReal\\nReal \\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": 1,\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 2,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ],\n",
            "          \"token_count\": 0,\n",
            "          \"grounding_attributions\": []\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 922,\n",
            "        \"candidates_token_count\": 44,\n",
            "        \"total_token_count\": 966\n",
            "      }\n",
            "    }),\n",
            ")\n",
            "Filtered results: ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Error: Expected 20 results but got 5. Continuing with next batch.\n",
            "Processed inputs 561 to 580, Results = ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Response: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=glm.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Fake\\nReal\\nFake\\nUndetermined\\nReal\\nFake\\nReal\\nFake\\nFake\\nReal\\nFake\\nReal\\nReal\\nReal\\nReal\\nReal\\nFake\\nFake\\nFake\\nReal \\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": 1,\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ],\n",
            "          \"token_count\": 0,\n",
            "          \"grounding_attributions\": []\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 1221,\n",
            "        \"candidates_token_count\": 40,\n",
            "        \"total_token_count\": 1261\n",
            "      }\n",
            "    }),\n",
            ")\n",
            "Filtered results: ['undetermined']\n",
            "Error: Expected 20 results but got 1. Continuing with next batch.\n",
            "Processed inputs 581 to 600, Results = ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Response: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=glm.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Undetermined\\nFake\\nFake\\nUndetermined\\nReal\\nFake\\nUndetermined\\nFake\\nFake\\nUndetermined\\nReal\\nFake\\nUndetermined\\nReal\\nReal\\nReal\\nReal\\nUndetermined\\nFake\\nUndetermined \\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": 1,\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ],\n",
            "          \"token_count\": 0,\n",
            "          \"grounding_attributions\": []\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 1018,\n",
            "        \"candidates_token_count\": 46,\n",
            "        \"total_token_count\": 1064\n",
            "      }\n",
            "    }),\n",
            ")\n",
            "Filtered results: ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Error: Expected 20 results but got 7. Continuing with next batch.\n",
            "Processed inputs 601 to 620, Results = ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Response: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=glm.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Undetermined\\nReal\\nReal\\nFake\\nFake\\nReal\\nReal\\nReal\\nReal\\nFake\\nReal\\nFake\\nFake\\nReal\\nFake\\nFake\\nReal\\nReal\\nReal\\nReal \\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": 1,\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ],\n",
            "          \"token_count\": 0,\n",
            "          \"grounding_attributions\": []\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 944,\n",
            "        \"candidates_token_count\": 40,\n",
            "        \"total_token_count\": 984\n",
            "      }\n",
            "    }),\n",
            ")\n",
            "Filtered results: ['undetermined']\n",
            "Error: Expected 20 results but got 1. Continuing with next batch.\n",
            "Processed inputs 621 to 640, Results = ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Response: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=glm.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"finish_reason\": 3,\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 3,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ],\n",
            "          \"token_count\": 0,\n",
            "          \"grounding_attributions\": []\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 914,\n",
            "        \"total_token_count\": 914,\n",
            "        \"candidates_token_count\": 0\n",
            "      }\n",
            "    }),\n",
            ")\n",
            "Error during detection: list index (0) out of range\n",
            "Processed inputs 641 to 660, Results = ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Response: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=glm.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Undetermined\\nUndetermined\\nUndetermined\\nUndetermined\\nFake\\nUndetermined\\nUndetermined\\nReal\\nFake\\nUndetermined\\nReal\\nReal\\nReal\\nReal\\nFake\\nUndetermined\\nReal\\nFake\\nFake\\nFake \\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": 1,\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 2,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 2,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 2,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ],\n",
            "          \"token_count\": 0,\n",
            "          \"grounding_attributions\": []\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 1043,\n",
            "        \"candidates_token_count\": 47,\n",
            "        \"total_token_count\": 1090\n",
            "      }\n",
            "    }),\n",
            ")\n",
            "Filtered results: ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Error: Expected 20 results but got 8. Continuing with next batch.\n",
            "Processed inputs 661 to 680, Results = ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Response: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=glm.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Fake\\nReal\\nReal\\nReal\\nReal\\nFake\\nReal\\nFake\\nReal\\nUndetermined\\nFake\\nFake\\nReal\\nReal\\nReal\\nFake\\nReal\\nFake\\nReal\\nReal \\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": 1,\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ],\n",
            "          \"token_count\": 0,\n",
            "          \"grounding_attributions\": []\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 1064,\n",
            "        \"candidates_token_count\": 40,\n",
            "        \"total_token_count\": 1104\n",
            "      }\n",
            "    }),\n",
            ")\n",
            "Filtered results: ['undetermined']\n",
            "Error: Expected 20 results but got 1. Continuing with next batch.\n",
            "Processed inputs 681 to 700, Results = ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Response: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=glm.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Fake\\nReal\\nFake\\nReal\\nReal\\nReal\\nReal\\nFake\\nReal\\nReal\\nFake\\nReal\\nReal\\nReal\\nReal\\nReal\\nFake\\nFake\\nReal\\nReal \\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": 1,\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ],\n",
            "          \"token_count\": 0,\n",
            "          \"grounding_attributions\": []\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 936,\n",
            "        \"candidates_token_count\": 39,\n",
            "        \"total_token_count\": 975\n",
            "      }\n",
            "    }),\n",
            ")\n",
            "Filtered results: []\n",
            "Error: Expected 20 results but got 0. Continuing with next batch.\n",
            "Processed inputs 701 to 720, Results = ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Response: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=glm.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Undetermined\\nUndetermined\\nReal\\nFake\\nFake\\nUndetermined\\nReal\\nUndetermined\\nUndetermined\\nReal\\nUndetermined\\nUndetermined\\nReal\\nFake\\nReal\\nUndetermined\\nFake\\nReal\\nReal\\nFake \\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": 1,\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ],\n",
            "          \"token_count\": 0,\n",
            "          \"grounding_attributions\": []\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 968,\n",
            "        \"candidates_token_count\": 47,\n",
            "        \"total_token_count\": 1015\n",
            "      }\n",
            "    }),\n",
            ")\n",
            "Filtered results: ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Error: Expected 20 results but got 8. Continuing with next batch.\n",
            "Processed inputs 721 to 740, Results = ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Response: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=glm.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Fake\\nUndetermined\\nFake\\nFake\\nReal\\nReal\\nReal\\nReal\\nReal\\nReal\\nUndetermined\\nFake\\nReal\\nFake\\nReal\\nReal\\nReal\\nReal\\nReal\\nReal \\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": 1,\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ],\n",
            "          \"token_count\": 0,\n",
            "          \"grounding_attributions\": []\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 1053,\n",
            "        \"candidates_token_count\": 41,\n",
            "        \"total_token_count\": 1094\n",
            "      }\n",
            "    }),\n",
            ")\n",
            "Filtered results: ['undetermined', 'undetermined']\n",
            "Error: Expected 20 results but got 2. Continuing with next batch.\n",
            "Processed inputs 741 to 760, Results = ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Response: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=glm.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Real\\nFake\\nReal\\nReal\\nReal\\nFake\\nFake\\nReal\\nReal\\nFake\\nReal\\nUndetermined\\nFake\\nFake\\nReal\\nReal\\nReal\\nFake\\nReal\\nReal \\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": 1,\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ],\n",
            "          \"token_count\": 0,\n",
            "          \"grounding_attributions\": []\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 1021,\n",
            "        \"candidates_token_count\": 40,\n",
            "        \"total_token_count\": 1061\n",
            "      }\n",
            "    }),\n",
            ")\n",
            "Filtered results: ['undetermined']\n",
            "Error: Expected 20 results but got 1. Continuing with next batch.\n",
            "Processed inputs 761 to 780, Results = ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Response: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=glm.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Undetermined\\nReal\\nFake\\nFake\\nReal\\nReal\\nReal\\nFake\\nFake\\nReal\\nFake\\nFake\\nReal\\nFake\\nReal\\nReal\\nFake\\nReal\\nFake\\nReal \\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": 1,\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ],\n",
            "          \"token_count\": 0,\n",
            "          \"grounding_attributions\": []\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 1183,\n",
            "        \"candidates_token_count\": 40,\n",
            "        \"total_token_count\": 1223\n",
            "      }\n",
            "    }),\n",
            ")\n",
            "Filtered results: ['undetermined']\n",
            "Error: Expected 20 results but got 1. Continuing with next batch.\n",
            "Processed inputs 781 to 800, Results = ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Response: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=glm.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Undetermined\\nFake\\nUndetermined\\nReal\\nFake\\nReal\\nFake\\nReal\\nReal\\nFake\\nReal\\nReal\\nReal\\nReal\\nReal\\nReal\\nFake\\nUndetermined\\nReal\\nFake \\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": 1,\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ],\n",
            "          \"token_count\": 0,\n",
            "          \"grounding_attributions\": []\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 1040,\n",
            "        \"candidates_token_count\": 42,\n",
            "        \"total_token_count\": 1082\n",
            "      }\n",
            "    }),\n",
            ")\n",
            "Filtered results: ['undetermined', 'undetermined', 'undetermined']\n",
            "Error: Expected 20 results but got 3. Continuing with next batch.\n",
            "Processed inputs 801 to 820, Results = ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Response: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=glm.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Real\\nReal\\nReal\\nReal\\nFake\\nReal\\nFake\\nReal\\nReal\\nReal\\nReal\\nReal\\nReal\\nFake\\nReal\\nReal\\nReal\\nReal\\nFake\\nReal \\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": 1,\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ],\n",
            "          \"token_count\": 0,\n",
            "          \"grounding_attributions\": []\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 1058,\n",
            "        \"candidates_token_count\": 39,\n",
            "        \"total_token_count\": 1097\n",
            "      }\n",
            "    }),\n",
            ")\n",
            "Filtered results: []\n",
            "Error: Expected 20 results but got 0. Continuing with next batch.\n",
            "Processed inputs 821 to 840, Results = ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Response: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=glm.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Real\\nFake\\nFake\\nReal\\nReal\\nFake\\nReal\\nFake\\nReal\\nFake\\nFake\\nFake\\nFake\\nReal\\nFake\\nFake\\nReal\\nFake\\nReal\\nReal \\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": 1,\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ],\n",
            "          \"token_count\": 0,\n",
            "          \"grounding_attributions\": []\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 1045,\n",
            "        \"candidates_token_count\": 39,\n",
            "        \"total_token_count\": 1084\n",
            "      }\n",
            "    }),\n",
            ")\n",
            "Filtered results: []\n",
            "Error: Expected 20 results but got 0. Continuing with next batch.\n",
            "Processed inputs 841 to 860, Results = ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Response: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=glm.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Fake\\nFake\\nFake\\nReal\\nFake\\nFake\\nReal\\nReal\\nFake\\nFake\\nFake\\nReal\\nReal\\nReal\\nReal\\nReal\\nReal\\nReal\\nReal\\nReal \\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": 1,\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ],\n",
            "          \"token_count\": 0,\n",
            "          \"grounding_attributions\": []\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 1033,\n",
            "        \"candidates_token_count\": 39,\n",
            "        \"total_token_count\": 1072\n",
            "      }\n",
            "    }),\n",
            ")\n",
            "Filtered results: []\n",
            "Error: Expected 20 results but got 0. Continuing with next batch.\n",
            "Processed inputs 861 to 880, Results = ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Response: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=glm.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Real\\nFake\\nReal\\nFake\\nFake\\nFake\\nReal\\nReal\\nFake\\nReal\\nFake\\nReal\\nFake\\nFake\\nFake\\nReal\\nReal\\nReal\\nFake\\nFake \\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": 1,\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ],\n",
            "          \"token_count\": 0,\n",
            "          \"grounding_attributions\": []\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 942,\n",
            "        \"candidates_token_count\": 39,\n",
            "        \"total_token_count\": 981\n",
            "      }\n",
            "    }),\n",
            ")\n",
            "Filtered results: []\n",
            "Error: Expected 20 results but got 0. Continuing with next batch.\n",
            "Processed inputs 881 to 900, Results = ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Response: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=glm.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Undetermined\\nUndetermined\\nFake\\nReal\\nUndetermined\\nFake\\nUndetermined\\nFake\\nReal\\nReal\\nUndetermined\\nReal\\nReal\\nReal\\nReal\\nUndetermined\\nReal\\nReal\\nReal\\nReal \\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": 1,\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ],\n",
            "          \"token_count\": 0,\n",
            "          \"grounding_attributions\": []\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 906,\n",
            "        \"candidates_token_count\": 45,\n",
            "        \"total_token_count\": 951\n",
            "      }\n",
            "    }),\n",
            ")\n",
            "Filtered results: ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Error: Expected 20 results but got 6. Continuing with next batch.\n",
            "Processed inputs 901 to 920, Results = ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Response: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=glm.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Undetermined\\nFake\\nUndetermined\\nFake\\nUndetermined\\nUndetermined\\nUndetermined\\nReal\\nReal\\nFake\\nFake\\nReal\\nReal\\nUndetermined\\nReal\\nFake\\nFake\\nReal\\nReal\\nReal \\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": 1,\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ],\n",
            "          \"token_count\": 0,\n",
            "          \"grounding_attributions\": []\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 1031,\n",
            "        \"candidates_token_count\": 45,\n",
            "        \"total_token_count\": 1076\n",
            "      }\n",
            "    }),\n",
            ")\n",
            "Filtered results: ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Error: Expected 20 results but got 6. Continuing with next batch.\n",
            "Processed inputs 921 to 940, Results = ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Response: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=glm.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Fake\\nReal\\nFake\\nReal\\nReal\\nReal\\nReal\\nReal\\nReal\\nFake\\nReal\\nReal\\nReal\\nFake\\nReal\\nFake\\nReal\\nFake\\nReal\\nReal \\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": 1,\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ],\n",
            "          \"token_count\": 0,\n",
            "          \"grounding_attributions\": []\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 917,\n",
            "        \"candidates_token_count\": 39,\n",
            "        \"total_token_count\": 956\n",
            "      }\n",
            "    }),\n",
            ")\n",
            "Filtered results: []\n",
            "Error: Expected 20 results but got 0. Continuing with next batch.\n",
            "Processed inputs 941 to 960, Results = ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Response: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=glm.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Real\\nFake\\nReal\\nUndetermined\\nReal\\nFake\\nReal\\nReal\\nReal\\nReal\\nFake\\nFake\\nFake\\nReal\\nReal\\nReal\\nFake\\nReal\\nReal\\nFake \\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": 1,\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ],\n",
            "          \"token_count\": 0,\n",
            "          \"grounding_attributions\": []\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 941,\n",
            "        \"candidates_token_count\": 40,\n",
            "        \"total_token_count\": 981\n",
            "      }\n",
            "    }),\n",
            ")\n",
            "Filtered results: ['undetermined']\n",
            "Error: Expected 20 results but got 1. Continuing with next batch.\n",
            "Processed inputs 961 to 980, Results = ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Response: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=glm.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"Fake\\nReal\\nFake\\nFake\\nReal\\nFake\\nReal\\nFake\\nReal\\nReal\\nFake\\nFake\\nReal\\nFake\\nFake\\nFake\\nFake\\nReal\\nReal\\nUndetermined \\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": 1,\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ],\n",
            "          \"token_count\": 0,\n",
            "          \"grounding_attributions\": []\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 1028,\n",
            "        \"candidates_token_count\": 40,\n",
            "        \"total_token_count\": 1068\n",
            "      }\n",
            "    }),\n",
            ")\n",
            "Filtered results: ['undetermined']\n",
            "Error: Expected 20 results but got 1. Continuing with next batch.\n",
            "Processed inputs 981 to 1000, Results = ['undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined', 'undetermined']\n",
            "Daily limit reached. Stopping execution.\n",
            "Processing complete. Results saved to processed_mendeley_set.csv\n"
          ]
        }
      ]
    }
  ]
}